{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a Vector Search Microservice with Amazon SageMaker\n",
    "\n",
    "This notebook shows how to deploy a vector search microservice using Amazon SageMaker's real time endpoint feature. The microservice uses FAISS for efficient similarity search combined with some of LangChain's useful document handlers and its FAISS wrapper. The embedding model which powers the similarity search is retrieved from the HuggingFace model hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup\n",
    "\n",
    "This section imports necessary AWS SDK libraries like boto3 and sagemaker. It also sets up the boto3 session to point to the right AWS credentials and resources.\n",
    "\n",
    "The local variable is used to determine if we are running in SageMaker Studio or not. If local is True, we use the AWS_PROFILE environment variable to configure boto3. The role variable is also configured to either use the SageMaker execution role or get it from Studio.\n",
    "\n",
    "Please note - the `SAGEMAKER_ROLE` variable must be an an execution role from SageMaker for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r src/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to True if running locally and not in sagemaker studio\n",
    "local = False\n",
    "if local:\n",
    "    boto3.setup_default_session(profile_name=os.environ['AWS_PROFILE'])\n",
    "    role = os.environ['SAGEMAKER_ROLE']\n",
    "else:\n",
    "    role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Documents with LangChain and FAISS\n",
    "\n",
    "This section prepares the example document (a sample of the Amazon SageMaker FAQ docs), embeds them with a HuggingFace model, and saves the embeddings to a FAISS index.\n",
    "\n",
    "This is all done with LangChain wrappers which we import below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Documents\n",
    "\n",
    "First, make sure to download the dataset to your local environment using the cell below. However, you are able to use any sort of text data here which you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = \"s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv\"\n",
    "!aws s3 cp $s3_path ./data/sagemaker_faqs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the example CSV file containing SageMaker FAQs and wrap the text with the Document class from LangChain.\n",
    "\n",
    "Since the document is large and we would not want to include the whole FAQ list as context to a large language model, we split it into smaller chunks using the CharacterTextSplitter. This is important because it will help decrease the input token count for a retrieval augmented generation (RAG) system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sagemaker FAQ list\n",
    "with open('./data/sagemaker_faqs.csv') as f:\n",
    "    doc = f.read()\n",
    "\n",
    "# create a loader\n",
    "docs = []\n",
    "loader = TextLoader('')\n",
    "docs.append(Document(page_content=doc))\n",
    "\n",
    "# split documents into chunks\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Store\n",
    "\n",
    "Finally, we create the FAISS index from the documents using the embeddings module. This stores the document embeddings for efficient similarity search later.\n",
    "\n",
    "Storing the embeddings in a vector store like FAISS allows us to quickly find similar passages by doing nearest neighbor search on the embedding index. This is the foundation for dense retrieval in systems like RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instantiation to embedding model\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_instruction=\"Represent this sentence for searching relevant passages: \"\n",
    ")\n",
    "\n",
    "# create vector store\n",
    "vs = FAISS.from_documents(split_docs, hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Vector Store Locally\n",
    "\n",
    "Once the documents have been indexed into FAISS, we will save the index locally into a directory called `faiss_vector_store`. You can see below that you are able to use the `load_local` method to create a FAISS in-memory index from this persisted vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.save_local('faiss_vector_store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = FAISS.load_local(\"faiss_vector_store\", hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Vector Store to a SageMaker Endpoint\n",
    "\n",
    "Now that we have created our vector store, let's go ahead and deploy it to a SageMaker endpoint!\n",
    "\n",
    "First, lets start by creating a SageMaker session and specifying an S3 bucket location to store our vector search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'faiss-demo-deploy'\n",
    "s3_uri = f's3://{bucket}/{prefix}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package the vector store as a tar file in S3\n",
    "\n",
    "The local directory containing the FAISS index we created earlier now has to be packaged into a tar file because SageMaker expects all model objects in tar.gz format. This process is similar to packaging a serialized machine learning model for SageMaker deployment; only in this case, we are using a FAISS index as our \"model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf model.tar.gz ./faiss_vector_store\n",
    "!tar -tvf model.tar.gz\n",
    "model_uri = sess.upload_data('model.tar.gz', bucket = bucket, key_prefix=f\"{prefix}/model\")\n",
    "!rm model.tar.gz\n",
    "!rm -rf faiss_vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a SageMaker Model Object\n",
    "\n",
    "Once the model artifact has been uploaded to S3, you will use the SageMaker SDK to create a model object which references the model artifact in S3, one of SageMaker's PyTorch inference containers, and the inference code stored in the `src` directory in this repository. The `inference.py` is the code which is executed at runtime while the `requirements.txt` tells SageMaker to install the necessary libraries inside its Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "image = sagemaker.image_uris.retrieve(\n",
    "    framework='pytorch',\n",
    "    region='us-east-1',\n",
    "    image_scope='inference',\n",
    "    version='1.12',\n",
    "    instance_type='ml.m5.2xlarge',\n",
    ")\n",
    "\n",
    "model_name = f'faiss-vs-{int(time.time())}'\n",
    "faiss_model_sm = sagemaker.model.Model(\n",
    "    model_data=model_uri,\n",
    "    image_uri=image,\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir='src',\n",
    "    name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Vector Store Endpoint\n",
    "\n",
    "Deploying the model object to sagemaker can be done with the deploy function. We will be using a CPU instance in this case. Make sure to scale this instance vertically for effective vector search processing and horizontally for load handling as required in a production system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "endpoint_name = f'faiss-endpoint-{int(time.time())}'\n",
    "faiss_model_sm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Retrieval from SageMaker Endpoint\n",
    "\n",
    "Once the model has deployed, you can connect to the endpoint using the `Predictor` class in the SageMaker SDK. This connection can then use the predict method in order to search input text against our SageMaker FAQ index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_vector_store = sagemaker.predictor.Predictor(endpoint_name)\n",
    "assert sagemaker_vector_store.endpoint_context().properties['Status'] == 'InService'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "payload = json.dumps({\n",
    "    \"text\": \"what is a shadow test?\",\n",
    "    \"k\": 3,\n",
    "})\n",
    "\n",
    "out = sagemaker_vector_store.predict(\n",
    "    payload,\n",
    "    initial_args={\"ContentType\": \"application/json\", \"Accept\": \"application/json\"}\n",
    ")\n",
    "out = json.loads(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output is the text chunks which closely match the input question. Just like that we have a retrieval system API up and running which can power a RAG based application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Clean Up Endpoint\n",
    "\n",
    "Once you have finished testing you endpoint, you have the option to delete your SageMaker endpoint. This is a good practice as experimental endpoints can be removed in order to decrease your SageMaker costs when they are not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_vector_store.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isengard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
